 # Dynamic Data Analysis Platform

***This project is under heavy development and is not yet ready for use.***

## ğŸ“Š Overview
A state-of-the-art, comprehensive platform for advanced automated data analysis that supports the complete analytics lifecycle: descriptive, diagnostic, [Future : predictive, and prescriptive analytics]. This powerful tool allows users to upload data in a variety of formats and seamlessly progress through each stage of data analysis with sophisticated algorithms and visualizations.

## ğŸŒŸ Features

### Data Ingestion
- **Multiple File Format Support**: Upload and analyze data from various formats:
  - [x] âœ… Excel (xlsx, xls)
  - [x] âœ… CSV
  - [x] âœ… JSON
  - Parquet
  - Avro
  - XML
  - ORC

**Engines**
-   [x] âœ… Pandas
-   [x] âœ… Polars
-   [ ] PySpark

**Preprocessing and Analysis**
- [x] âœ… **Descriptive Analytics**  
  Gain insights into past data with detailed summaries and visualizations.
  
- [ ] ğŸ”„ **Diagnostic Analytics**  
  Identify the root causes of trends and anomalies in your data.

- [ ] ğŸ”„ **Predictive Analytics** *(Coming Soon)*  
  Use machine learning models to forecast future outcomes.

- [ ] ğŸ”„ **Prescriptive Analytics** *(Coming Soon)*  
  Get recommendations for decision-making based on predictive analytics.

- **Data Validation**: Automatic validation and error detection in uploaded data
- **Integration**: Connect directly with databases and APIs

- **Mutiple Engine Support**: Pandas, Polars, PySpark

### Data Processing
- **Automated Data Cleaning**: Smart detection and handling of missing values, outliers, and duplicates
- **Data Transformation**: Feature engineering, normalization, and encoding
- **Data Enrichment**: Merge external data sources for enhanced analysis

### Advanced Analytics Suite
- **Descriptive Analytics**: Understand what happened
  - Statistical summaries
  - Distribution analysis
  - Correlation matrices
  - Pattern identification
  
- **Diagnostic Analytics**: Understand why it happened
  - Root cause analysis
  - Anomaly detection
  - Variance analysis
  - Segmentation
  
- **Predictive Analytics**: Understand what might happen
  - Time series forecasting
  - Regression analysis
  - Classification models
  - Clustering
  
- **Prescriptive Analytics**: Understand what actions to take
  - Optimization algorithms
  - Simulation modeling
  - Decision trees
  - Recommendation engines

### Visualization & Reporting
- **Interactive Dashboards**: Drag-and-drop interface to create custom dashboards
- **Advanced Visualizations**: From simple charts to complex network graphs
- **Report Generation**: Export findings as PDF, PowerPoint, or interactive HTML [Advance Feature]

### Multiple Interfaces
- **Web UI**: User-friendly interface for non-technical users, [POC : Streamlit , Advance Feature : Next.js] 
- **REST API**: Programmatic access for integration with other systems
- **CLI**: Command-line tools for automation and scripting


## Core Objective of the Project.
## ğŸ” Step-by-Step Analysis Capabilities

The platform guides users through a structured data analysis workflow:

### 1. Descriptive Analysis
- Data profiling and summary statistics [x]
- Correlation analysis [x] and relationship mapping
- Trend identification and pattern recognition
- Distribution analysis and visualization [x]
- Time-series decomposition (trend, seasonality, residual) [Advance Feature]
- Geospatial analysis for location-based insights [Advance Feature]

### 2. Diagnostic Analysis
- Drill-down capabilities for deeper investigation
- Anomaly detection and outlier analysis
- Comparative analysis across dimensions
- Factor analysis to determine influence weights
- Causal inference techniques (e.g., Granger causality, A/B testing) [Advance Feature]
- Hypothesis testing for statistical validation [Advance Feature]

### 3. Predictive Analysis [Advance Feature]
- Automated model selection and hyperparameter tuning
- Feature importance ranking
- Cross-validation and model evaluation
- Confidence intervals and prediction probabilities
- Ensemble methods (e.g., Random Forest, Gradient Boosting) 
- Deep learning models for unstructured data 
- NLP techniques for text-based predictions

### 4. Prescriptive Analysis [Advance Feature]
- "What-if" scenario modeling
- Optimization for business objectives
- Action recommendation prioritization
- Risk assessment and mitigation strategies  
- Real-time optimization for dynamic decision-making  
- Game theory models for competitive strategy analysis  
- Cost-benefit analysis for prioritizing recommendations 

## ğŸš€ Installation and Setup

### Prerequisites
- Python 3.13 
- Git

### Using pip with uv (recommended)
```bash
# Install uv if not already installed
pip install uv

# Clone the repository
git clone https://github.com/AashishTangnami/data_analysis_tool.git
cd data_analysis_tool

# Install the package
uv pip install -e .

# For development
uv pip install -e ".[dev]"
```

### Environment Variables
Create a `.env` file in the root directory based on the provided `.env.example`:
```
API_KEY=your_api_key
DATABASE_URL=your_database_url
```

## ğŸ“Š Usage Examples

### Command Line Interface (CLI)
```bash
-------
```

### Web Interface
1. Start the web server:
   ```bash
   ------
   ```
2. Open your browser at http://localhost:8000
3. Upload your data file
4. Navigate through the analysis tabs

### API Usage
```python
import requests

# Upload file
files = {'file': open('data.csv', 'rb')}
response = requests.post('http://localhost:8000/api/upload', files=files)
file_id = response.json()['file_id']

# Get descriptive analysis
analysis = requests.get(f'http://localhost:8000/api/analysis/{file_id}/descriptive')
print(analysis.json())
```

## ğŸ‘¨â€ğŸ’» Development

### Project Structure
```
dynamic_data_analysis_platform/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ cache/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ transformation.py
â”‚   â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ requests.py
â”‚   â”‚   â”‚       â””â”€â”€ responses.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ core/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ context.py
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ analysis/
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ descriptive/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ pandas_descriptive.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ polars_descriptive.py
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ pyspark_descriptive.py
â”‚   â”‚       â”‚   â”œâ”€â”€ diagnostic/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ pandas_diagnostic.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ polars_diagnostic.py
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ pyspark_diagnostic.py
â”‚   â”‚       â”‚   â”œâ”€â”€ predictive/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ pandas_predictive.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ polars_predictive.py
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ pyspark_predictive.py
â”‚   â”‚       â”‚   â””â”€â”€ prescriptive/
â”‚   â”‚       â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚       â”œâ”€â”€ base.py
â”‚   â”‚       â”‚       â”œâ”€â”€ pandas_prescriptive.py
â”‚   â”‚       â”‚       â”œâ”€â”€ polars_prescriptive.py
â”‚   â”‚       â”‚       â””â”€â”€ pyspark_prescriptive.py
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ engines/
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ base.py
â”‚   â”‚       â”‚   â”œâ”€â”€ pandas_engine.py
â”‚   â”‚       â”‚   â”œâ”€â”€ polars_engine.py
â”‚   â”‚       â”‚   â””â”€â”€ pyspark_engine.py
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ ingestion/
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ base.py
â”‚   â”‚       â”‚   â”œâ”€â”€ pandas_ingestion.py
â”‚   â”‚       â”‚   â”œâ”€â”€ polars_ingestion.py
â”‚   â”‚       â”‚   â””â”€â”€ pyspark_ingestion.py
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ preprocessing/
â”‚   â”‚           â”œâ”€â”€ __init__.py
â”‚   â”‚           â”œâ”€â”€ base.py
â”‚   â”‚           â”œâ”€â”€ pandas_preprocessing.py
â”‚   â”‚           â”œâ”€â”€ polars_preprocessing.py
â”‚   â”‚           â””â”€â”€ pyspark_preprocessing.py
â”‚   â”‚
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py
â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â”‚   â””â”€â”€ upload.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ analysis_rerenders.py
â”‚   â”‚       â”œâ”€â”€ data_preview.py
â”‚   â”‚       â”œâ”€â”€ engine_selector.py
â”‚   â”‚       â”œâ”€â”€ file_uploader.py
â”‚   â”‚       â””â”€â”€ visualization.py
â”‚   â”‚

------------ Additional ----------
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ commands.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ file_handlers.py
â”‚       â”œâ”€â”€ data_validators.py
â”‚       â””â”€â”€ logging_config.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_data/
â”‚   â”‚   â”œâ”€â”€ sample_csv.csv
â”‚   â”‚   â”œâ”€â”€ sample_json.json
â”‚   â”‚   â””â”€â”€ sample_excel.xlsx
â”‚   â”‚
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ test_data_preprocessing.py
â”‚   â”‚   â”œâ”€â”€ test_descriptive_analysis.py
â”‚   â”‚   â”œâ”€â”€ test_diagnostic_analysis.py
â”‚   â”‚   â”œâ”€â”€ test_predictive_analysis.py
â”‚   â”‚   â””â”€â”€ test_prescriptive_analysis.py
â”‚   â”‚
â”‚   â””â”€â”€ integration/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_api.py
â”‚       â”œâ”€â”€ test_frontend.py
â”‚       â””â”€â”€ test_end_to_end.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api_documentation.md
â”‚   â”œâ”€â”€ user_guide.md
â”‚   â””â”€â”€ developer_guide.md
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ start_api.sh
    â”œâ”€â”€ start_frontend.sh
    â””â”€â”€ run_tests.sh
```

### Running Tests
```bash
pytest
```

### Contributing
1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit your changes: `git commit -am 'Add feature'`
4. Push to the branch: `git push origin feature-name`
5. Submit a pull request

## ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Support
For support and questions, please open an issue on the GitHub repository.
